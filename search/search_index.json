{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pytorch_Ext Extension to Pytorch, including new modules, new functionals, bug fixes, etc. Install Install from source to get the up-to-date version: pip install git+https://github.com/david-leon/Pytorch_Ext.git dependency Pytorch >=1.0 Project Layout Python Module Explanation module all neual network module definitions functional operations on tensor with no parameter to be learned util utility functions","title":"Home"},{"location":"#pytorch_ext","text":"Extension to Pytorch, including new modules, new functionals, bug fixes, etc.","title":"Pytorch_Ext"},{"location":"#install","text":"Install from source to get the up-to-date version: pip install git+https://github.com/david-leon/Pytorch_Ext.git","title":"Install"},{"location":"#dependency","text":"Pytorch >=1.0","title":"dependency"},{"location":"#project-layout","text":"Python Module Explanation module all neual network module definitions functional operations on tensor with no parameter to be learned util utility functions","title":"Project Layout"},{"location":"functional/","text":"dropout Dropout function for any dimension input. dropout(input, p=0.5, shared_axes=(), rescale=True) input : tensor to be applied. p : float, probability to drop a value (set to zero) shared_axes : tuple of int, axes to share the dropout mask over. By default, each value is dropped individually. For example, shared_axes=(0,) means using the same mask across the batch. shared_axes=(2, 3) means using the same mask across the spatial dimensions of 2D feature maps, i.e., drop channels. rescale : if True (default), the input tensor will be rescaled by 1-p to compensate mean fluctuation due to dropout","title":"Functional"},{"location":"functional/#dropout","text":"Dropout function for any dimension input. dropout(input, p=0.5, shared_axes=(), rescale=True) input : tensor to be applied. p : float, probability to drop a value (set to zero) shared_axes : tuple of int, axes to share the dropout mask over. By default, each value is dropped individually. For example, shared_axes=(0,) means using the same mask across the batch. shared_axes=(2, 3) means using the same mask across the spatial dimensions of 2D feature maps, i.e., drop channels. rescale : if True (default), the input tensor will be rescaled by 1-p to compensate mean fluctuation due to dropout","title":"dropout"},{"location":"history/","text":"History version 0.9.0 [11-24-2020] NEW : add online documentation NEW : add functional.dropout() version 0.8.0 [9-8-2020] NEW : add BatchNorm module for replacing pytorch's implementation version 0.7.6 [5-7-2020] MODIFIED : add support for gzip compression for gpickle.loads() & gpickle.dumps() functions version 0.7.5 [1-8-2020] NEW : add util.get_file_md5() for file md5 check version 0.7.4 [1-3-2020] NEW : add util.set_value() for tensor initialization with numpy array version 0.7.3 [12-26-2019] NEW : Add util.freeze_module()/unfreeze_module()/get_trainable_parameters() for training parameters handling version 0.7.2 [11-5-2019] NEW : Add util.verbose_print() for print with verbose level filtering","title":"History"},{"location":"history/#history","text":"","title":"History"},{"location":"history/#version-090-11-24-2020","text":"NEW : add online documentation NEW : add functional.dropout()","title":"version 0.9.0 [11-24-2020]"},{"location":"history/#version-080-9-8-2020","text":"NEW : add BatchNorm module for replacing pytorch's implementation","title":"version 0.8.0 [9-8-2020]"},{"location":"history/#version-076-5-7-2020","text":"MODIFIED : add support for gzip compression for gpickle.loads() & gpickle.dumps() functions","title":"version 0.7.6 [5-7-2020]"},{"location":"history/#version-075-1-8-2020","text":"NEW : add util.get_file_md5() for file md5 check","title":"version 0.7.5 [1-8-2020]"},{"location":"history/#version-074-1-3-2020","text":"NEW : add util.set_value() for tensor initialization with numpy array","title":"version 0.7.4 [1-3-2020]"},{"location":"history/#version-073-12-26-2019","text":"NEW : Add util.freeze_module()/unfreeze_module()/get_trainable_parameters() for training parameters handling","title":"version 0.7.3 [12-26-2019]"},{"location":"history/#version-072-11-5-2019","text":"NEW : Add util.verbose_print() for print with verbose level filtering","title":"version 0.7.2 [11-5-2019]"},{"location":"module/","text":"BatchNorm Batch normalization for any dimention input, adapted from Dandelion 's BatchNorm class. The normalization is done as $$ \\begin{align} x' = \\gamma * \\frac{(x-\\mu)}{\\sigma} + \\beta \\end{align} $$ You can fabricate nonstandard BN variant by diabling any parameter among { \\mu \\mu , \\sigma \\sigma , \\gamma \\gamma , \\beta \\beta } class BatchNorm(input_shape=None, axes='auto', eps=1e-5, alpha=0.01, beta=0.0, gamma=1.0, mean=0.0, inv_std=1.0) input_shape : tuple or list of ints or tensor. Input shape of BatchNorm module, including batch dimension. axes : auto or tuple of int. The axis or axes to normalize over. If auto (the default), normalize over all axes except for the second: this will normalize over the minibatch dimension for dense layers, and additionally over all spatial dimensions for convolutional layers. eps : small constant \ud835\udf16 added to the variance before taking the square root and dividing by it, to avoid numerical problems alpha : coefficient for the exponential moving average of batch-wise means and standard deviations computed during training; the closer to one, the more it will depend on the last batches seen gamma, beta : these two parameters can be set to None to disable the controversial scale and shift as well as save computing power. According to Deep Learning Book, Section 8.7.1 , disabling \\gamma \\gamma and \\beta \\beta might reduce the expressive power of the neural network. mean, inv_std : initial values for \\mu \\mu and \\frac{1}{\\sigma} \\frac{1}{\\sigma} . These two parameters can also be set to None to diable the mean substraction and variance scaling. .forward(x) Use self.training attribute to switch between training mode and inference mode. Center Estimate class centers by moving averaging, adapted from Dandelion 's Center class class Center(feature_dim, center_num, alpha=0.9, centers=None) feature_dim : feature dimension center_num : class center number center : initialization of class centers, should be in shape of (center_num, feature_dim) alpha : moving averaging coefficient, the closer to one, the more it will depend on the last batches seen: C_{new} = \\alpha*C_{batch} + (1-\\alpha)*C_{old} C_{new} = \\alpha*C_{batch} + (1-\\alpha)*C_{old} .forward(features=None, labels=None) features : batch features, from which the class centers will be estimated labels : features 's corresponding class labels return : centers estimated. Use self.training attribute to switch between training mode and inference mode. In training mode, features and labels are required for input; in inference mode these inputs will be ignored.","title":"Module"},{"location":"module/#batchnorm","text":"Batch normalization for any dimention input, adapted from Dandelion 's BatchNorm class. The normalization is done as $$ \\begin{align} x' = \\gamma * \\frac{(x-\\mu)}{\\sigma} + \\beta \\end{align} $$ You can fabricate nonstandard BN variant by diabling any parameter among { \\mu \\mu , \\sigma \\sigma , \\gamma \\gamma , \\beta \\beta } class BatchNorm(input_shape=None, axes='auto', eps=1e-5, alpha=0.01, beta=0.0, gamma=1.0, mean=0.0, inv_std=1.0) input_shape : tuple or list of ints or tensor. Input shape of BatchNorm module, including batch dimension. axes : auto or tuple of int. The axis or axes to normalize over. If auto (the default), normalize over all axes except for the second: this will normalize over the minibatch dimension for dense layers, and additionally over all spatial dimensions for convolutional layers. eps : small constant \ud835\udf16 added to the variance before taking the square root and dividing by it, to avoid numerical problems alpha : coefficient for the exponential moving average of batch-wise means and standard deviations computed during training; the closer to one, the more it will depend on the last batches seen gamma, beta : these two parameters can be set to None to disable the controversial scale and shift as well as save computing power. According to Deep Learning Book, Section 8.7.1 , disabling \\gamma \\gamma and \\beta \\beta might reduce the expressive power of the neural network. mean, inv_std : initial values for \\mu \\mu and \\frac{1}{\\sigma} \\frac{1}{\\sigma} . These two parameters can also be set to None to diable the mean substraction and variance scaling. .forward(x) Use self.training attribute to switch between training mode and inference mode.","title":"BatchNorm"},{"location":"module/#center","text":"Estimate class centers by moving averaging, adapted from Dandelion 's Center class class Center(feature_dim, center_num, alpha=0.9, centers=None) feature_dim : feature dimension center_num : class center number center : initialization of class centers, should be in shape of (center_num, feature_dim) alpha : moving averaging coefficient, the closer to one, the more it will depend on the last batches seen: C_{new} = \\alpha*C_{batch} + (1-\\alpha)*C_{old} C_{new} = \\alpha*C_{batch} + (1-\\alpha)*C_{old} .forward(features=None, labels=None) features : batch features, from which the class centers will be estimated labels : features 's corresponding class labels return : centers estimated. Use self.training attribute to switch between training mode and inference mode. In training mode, features and labels are required for input; in inference mode these inputs will be ignored.","title":"Center"},{"location":"util/","text":"freeze_module Freeze a module during training freeze_module(module) module : instance of torch.nn.Module return : no return unfreeze_module Un-freeze a module for training unfreeze_module(module) module : instance of torch.nn.Module return : no return get_trainable_parameters Retrieve only trainable parameters, for feeding optimizer get_trainable_parameters(module, with_name=False) module : instance of torch.nn.Module with_name : if True , output in format of (name, tensor), else only tensor returned return : generator of trainable parameters set_value Set tensor value with numpy array set_value(t, v) t : tensor v : numpy array return : no return get_device Retrieve device from tensor or module get_device(x) x : tensor or instance of nn.Module return : torch.device gpickle Pickle with gzip compression enabled. .dump(data, filename, compresslevel=9, protocol=4) Dump data and save to file. data : data to be dumped to file filename : file path compresslevel : gzip compression level, default = 9. protocol : protocol version of pickle, defalut = 4. .load(filename) Load dumped data from file filename : file to be loaded return : data unpickled .dumps(data, compresslevel=9, protocol=4) Dump data into bytes return : data pickled & compressed into bytes .loads(zipped_bytes) Load dumped data from bytes return : data unpickled verbose_print print with verbose level filtering class verbose_print(level=0, prefix=None) level : predefined verbose level. Instance of verbose_print functions the same with python's builtin print() with an additional l arg (default = 0); when l < this predefined verbose level, the print content will be suppressed, thus only content with verbose level >= level can be actually printed on screen. prefix : if given, each print will be preceded by this fixed prefix. Example vprint = verbose_print(level=2, prefix='LMExp') vprint('this line will be actually printed', l=3) vprint('this line will NOT be printed by verbose level filtering', l=0)","title":"Util"},{"location":"util/#freeze_module","text":"Freeze a module during training freeze_module(module) module : instance of torch.nn.Module return : no return","title":"freeze_module"},{"location":"util/#unfreeze_module","text":"Un-freeze a module for training unfreeze_module(module) module : instance of torch.nn.Module return : no return","title":"unfreeze_module"},{"location":"util/#get_trainable_parameters","text":"Retrieve only trainable parameters, for feeding optimizer get_trainable_parameters(module, with_name=False) module : instance of torch.nn.Module with_name : if True , output in format of (name, tensor), else only tensor returned return : generator of trainable parameters","title":"get_trainable_parameters"},{"location":"util/#set_value","text":"Set tensor value with numpy array set_value(t, v) t : tensor v : numpy array return : no return","title":"set_value"},{"location":"util/#get_device","text":"Retrieve device from tensor or module get_device(x) x : tensor or instance of nn.Module return : torch.device","title":"get_device"},{"location":"util/#gpickle","text":"Pickle with gzip compression enabled. .dump(data, filename, compresslevel=9, protocol=4) Dump data and save to file. data : data to be dumped to file filename : file path compresslevel : gzip compression level, default = 9. protocol : protocol version of pickle, defalut = 4. .load(filename) Load dumped data from file filename : file to be loaded return : data unpickled .dumps(data, compresslevel=9, protocol=4) Dump data into bytes return : data pickled & compressed into bytes .loads(zipped_bytes) Load dumped data from bytes return : data unpickled","title":"gpickle"},{"location":"util/#verbose_print","text":"print with verbose level filtering class verbose_print(level=0, prefix=None) level : predefined verbose level. Instance of verbose_print functions the same with python's builtin print() with an additional l arg (default = 0); when l < this predefined verbose level, the print content will be suppressed, thus only content with verbose level >= level can be actually printed on screen. prefix : if given, each print will be preceded by this fixed prefix. Example vprint = verbose_print(level=2, prefix='LMExp') vprint('this line will be actually printed', l=3) vprint('this line will NOT be printed by verbose level filtering', l=0)","title":"verbose_print"}]}